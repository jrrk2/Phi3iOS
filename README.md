This App implements the Phi3 LLM inference model on late model iPhones and iPads in an offline mode. Therefore it bypasses privacy and network
availability concerns. The initial install is large (2.5GB) but uses no network bandwidth once loaded. Suitable for iPhones such as the 16e with
8GB of RAM and hardware AI inference engine. Some limited functionality on lesser models.
![screenshot1](https://github.com/user-attachments/assets/915d491e-a8bc-415c-afea-60472cc98750)
![screenshot2](https://github.com/user-attachments/assets/a16eb3df-20bc-48ad-b672-45b5504115e1)
<img width="1024" alt="ipad" src="https://github.com/user-attachments/assets/719cd964-2612-46db-9962-4a5ae2026972" />
